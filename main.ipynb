{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try more hyperparams\n",
    "# TODO: try more tokens\n",
    "# TODO: add more data, i.e. bookcorpus/commoncrawl\n",
    "# TODO: make it easier to load saved models\n",
    "# TODO: log more info in wandb to make it easier to compare runs\n",
    "# (subclass trainer to do that?)\n",
    "\n",
    "%env WANDB_PROJECT=PACNLM\n",
    "%env WANDB_START_METHOD=thread\n",
    "\n",
    "import os\n",
    "import atexit\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import transformers.trainer as trainer\n",
    "from datasets import load_dataset\n",
    "from magic_timer import MagicTimer\n",
    "from magic_timer.format_seconds import format_seconds\n",
    "from tokenizers import normalizers\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    PrinterCallback,\n",
    "    RobertaPreLayerNormConfig,\n",
    "    RobertaPreLayerNormForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers.trainer import SequentialSampler\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "def sampler_monkey_patch(dataset, generator):\n",
    "    # When the dataset size is large, I've measured:\n",
    "    # RandomSampler -> ~50 samples/sec\n",
    "    # SequentialSampler -> ~500 samples/sec\n",
    "    # So this patch is to get a nearly 10x speedup...\n",
    "    # This has got training time on wikipedia from 1+ day to 3 hrs...\n",
    "    print(\"Monkey patching random sampler...\")\n",
    "    return SequentialSampler(dataset)\n",
    "\n",
    "\n",
    "trainer.RandomSampler = sampler_monkey_patch\n",
    "\n",
    "savedir = Path(os.environ[\"SAVEDIR\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1440 chars (roberta-base is 2048)\n",
      "accumulate every 28\n"
     ]
    }
   ],
   "source": [
    "# Roberta-base has 514 input tokens, which is 512 * 4 ~> 2048 chars\n",
    "NGRAM_SIZE: int = 6\n",
    "MAX_SEQ_LEN: int = NGRAM_SIZE * 128  # multiple of NGRAM_SIZE\n",
    "NUM_ATTENTION_HEADS: int = 12\n",
    "HIDDEN_SIZE: int = 768  # multiple of NUM_ATTENTION_HEADS, 768 default\n",
    "PROB_MASK: float = 0.15\n",
    "NUM_HIDDEN_LAYERS: int = 6\n",
    "\n",
    "TRAINING_BATCH_SIZE = 125\n",
    "DATALOADER_NUM_WORKERS = 10\n",
    "LEARNING_RATE = 3e-4  # defaults to 5e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "print(f\"Input length: {NGRAM_SIZE * MAX_SEQ_LEN} chars (roberta-base is 2048)\")\n",
    "\n",
    "assert HIDDEN_SIZE % NGRAM_SIZE == 0\n",
    "\n",
    "num_accumulation_steps = int(1200 / TRAINING_BATCH_SIZE)\n",
    "print(f\"accumulate every {num_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS = \"<cls>\"\n",
    "EOS = \"<eos>\"\n",
    "CHAR_PAD = \"<char_pad>\"\n",
    "UNK = \"<unk>\"\n",
    "NGRAM_PAD = \"<ngram_pad>\"\n",
    "MASK = \"<mask>\"\n",
    "\n",
    "SPECIAL_CHARS = {\n",
    "    CLS,\n",
    "    EOS,\n",
    "    CHAR_PAD,\n",
    "    UNK,\n",
    "    NGRAM_PAD,\n",
    "    MASK,\n",
    "}\n",
    "CHAR_TOKENS: list[str] = sorted(list(string.printable) + list(SPECIAL_CHARS))\n",
    "\n",
    "num_chars = len(CHAR_TOKENS)\n",
    "char_to_idx = {c: i for i, c in enumerate(CHAR_TOKENS)}\n",
    "idx_to_char = {i: c for i, c in enumerate(CHAR_TOKENS)}\n",
    "\n",
    "normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq: str):\n",
    "    seq = normalizer.normalize_str(seq)\n",
    "    seq = [CLS] + list(seq)\n",
    "    # Pad such that len(seq) is divisible by NGRAM_SIZE\n",
    "    if len(seq) % NGRAM_SIZE > 0:\n",
    "        seq += [CHAR_PAD] * (NGRAM_SIZE - (len(seq) % NGRAM_SIZE))\n",
    "    seq += [EOS] * NGRAM_SIZE\n",
    "    return torch.tensor(\n",
    "        [char_to_idx[c] if c in char_to_idx else char_to_idx[UNK] for c in seq]\n",
    "    )\n",
    "\n",
    "\n",
    "def collate(tokenized_seqs: list[torch.tensor], masking_probability: float = PROB_MASK):\n",
    "    \"\"\"Pad short seqs, truncate long seqs.\"\"\"\n",
    "    tokenized_seqs = [x[:MAX_SEQ_LEN] for x in tokenized_seqs]\n",
    "    max_len = max(x.shape[-1] for x in tokenized_seqs)\n",
    "    labels = torch.full(\n",
    "        size=[len(tokenized_seqs), max_len],\n",
    "        fill_value=char_to_idx[NGRAM_PAD],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    attention_mask = torch.ones_like(labels)\n",
    "    for i, x in enumerate(tokenized_seqs):\n",
    "        labels[i, 0 : len(x)] = x\n",
    "        attention_mask[i, len(x) :] = 0\n",
    "    # Masking, on ngram level rather than char\n",
    "    masked_labels = labels.clone().detach()\n",
    "    for row_idx in range(masked_labels.shape[0]):\n",
    "        for ngram_idx in range(0, masked_labels.shape[1], NGRAM_SIZE):\n",
    "            if random.random() < masking_probability:\n",
    "                masked_labels[\n",
    "                    row_idx, ngram_idx : ngram_idx + NGRAM_SIZE\n",
    "                ] = char_to_idx[MASK]\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"masked_labels\": masked_labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "def decode(labels):\n",
    "    # To convert back to text\n",
    "    predicted_sentences = []\n",
    "    for sentence_ids in labels:\n",
    "        chars = []\n",
    "        for i in sentence_ids:\n",
    "            char = idx_to_char[i]\n",
    "            # `char in chars[-1:]` is to compare to the last char,\n",
    "            # that also works when there are no no chars..\n",
    "            if char in SPECIAL_CHARS and char in chars[-1:]:\n",
    "                continue\n",
    "            chars.append(char)\n",
    "        predicted_sentences.append(\"\".join(chars))\n",
    "    return predicted_sentences\n",
    "\n",
    "\n",
    "class CharModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # An embedding table for each slot in the the ngram, (e.g. 0, 1, 2 for a NGRAM_SIZE=3).\n",
    "        self.ngram_embedding_tables = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Embedding(\n",
    "                    num_embeddings=num_chars,\n",
    "                    # embedding_dim=HIDDEN_SIZE,\n",
    "                    # # for concat varient:\n",
    "                    embedding_dim=HIDDEN_SIZE // NGRAM_SIZE,\n",
    "                    padding_idx=char_to_idx[NGRAM_PAD],\n",
    "                )\n",
    "                for _ in range(NGRAM_SIZE)\n",
    "            ]\n",
    "        )\n",
    "        self.language_model = RobertaPreLayerNormForMaskedLM(\n",
    "            config=RobertaPreLayerNormConfig(\n",
    "                vocab_size=2,  # won't use\n",
    "                hidden_size=HIDDEN_SIZE,  # default 768\n",
    "                max_position_embeddings=514,\n",
    "                num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "                type_vocab_size=1,\n",
    "                # cramming turns off dropout\n",
    "                attention_probs_dropout_prob=0,\n",
    "                hidden_dropout_prob=0,\n",
    "            )\n",
    "        )\n",
    "        # To map from the lm embeddings back to the chars\n",
    "        self.ngram_prediction_heads = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(HIDDEN_SIZE, num_chars) for _ in range(NGRAM_SIZE)]\n",
    "        )\n",
    "\n",
    "    def forward(self, labels, masked_labels, attention_mask):\n",
    "        logits = self.predict(masked_labels, attention_mask)[0]\n",
    "        loss = self.get_loss(logits, labels, attention_mask)\n",
    "        return MaskedLMOutput(loss=loss, logits=logits)\n",
    "\n",
    "    def predict(self, labels, attention_mask):\n",
    "        input_embeddings = self.get_input_embeddings(labels)\n",
    "        lm_embeddings = self.language_model.roberta_prelayernorm.forward(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=attention_mask[:, ::NGRAM_SIZE],\n",
    "        ).last_hidden_state\n",
    "        logits = self.get_predicted_char_logits(lm_embeddings)\n",
    "        return logits, lm_embeddings, input_embeddings\n",
    "\n",
    "    def get_loss(self, logits, labels, attention_mask):\n",
    "        loss_array = torch.nn.functional.cross_entropy(\n",
    "            logits.reshape(-1, num_chars), labels.reshape(-1), reduction=\"none\"\n",
    "        ) * attention_mask.reshape(-1)\n",
    "        return torch.mean(loss_array, dim=0)\n",
    "\n",
    "    def get_input_embeddings(self, x_batch: torch.tensor):\n",
    "        result = []\n",
    "        for ngram_slot_idx in range(NGRAM_SIZE):\n",
    "            ngram_slot_embeddings = self.ngram_embedding_tables[ngram_slot_idx](\n",
    "                x_batch[:, ngram_slot_idx::NGRAM_SIZE]\n",
    "            )\n",
    "            result.append(ngram_slot_embeddings)\n",
    "        # result = torch.stack(result).sum(dim=0)\n",
    "        # for concat varient:\n",
    "        result = torch.concatenate(result, dim=2)\n",
    "        return result\n",
    "\n",
    "    def get_predicted_char_logits(self, xbatch_lm_embeddings: torch.tensor):\n",
    "        \"\"\"Map from the lm embeddings back to the chars\"\"\"\n",
    "        result = []\n",
    "        for ngram_slot_idx in range(NGRAM_SIZE):\n",
    "            predicted_char = self.ngram_prediction_heads[ngram_slot_idx](\n",
    "                xbatch_lm_embeddings\n",
    "            )\n",
    "            result.append(predicted_char)\n",
    "        result = torch.concatenate(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        for x in self.ngram_embedding_tables:\n",
    "            x.to(*args, **kwargs)\n",
    "        self.language_model.to(*args, **kwargs)\n",
    "        for x in self.ngram_prediction_heads:\n",
    "            x.to(*args, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.examples = load_dataset(\n",
    "            \"wikipedia\",\n",
    "            \"20220301.en\",\n",
    "            split=split,\n",
    "            cache_dir=\"/media/bigdata/datasets/\",\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.examples[i][\"text\"]\n",
    "        if not type(text) == str:\n",
    "            text = \"\"\n",
    "        return tokenize(text)\n",
    "\n",
    "\n",
    "# dataset_train = MyDataset(split=f\"train[:1000]\")\n",
    "dataset_train = MyDataset(split=f\"train[:-{TRAINING_BATCH_SIZE}]\")\n",
    "dataset_eval = MyDataset(split=f\"train[-{TRAINING_BATCH_SIZE}:]\")\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel()\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PrinterCallback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyCallback\u001b[39;00m(PrinterCallback):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PrinterCallback' is not defined"
     ]
    }
   ],
   "source": [
    "class MyCallback(PrinterCallback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.total_timer = None\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called after logging the last logs.\n",
    "        \"\"\"\n",
    "        if not self.total_timer:\n",
    "            self.total_timer = MagicTimer()\n",
    "        eta = format_seconds(\n",
    "            (state.max_steps - state.global_step)\n",
    "            * (self.total_timer.time_elapsed() / state.global_step)\n",
    "        )\n",
    "        samples_per_second = (\n",
    "            state.global_step * TRAINING_BATCH_SIZE * num_accumulation_steps\n",
    "        ) / self.total_timer.time_elapsed()\n",
    "        print(\n",
    "            f\"{time.strftime('%Y%m%d-%H%M')}\"\n",
    "            f\" -- Time elapsed: {self.total_timer}\"\n",
    "            f\" -- Steps: {state.global_step} / {state.max_steps}\"\n",
    "            f\" -- Estimated time left: {eta}\"\n",
    "            f\" -- Samples per second: {samples_per_second}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# source: github.com/JonasGeiping/cramming\n",
    "def get_one_cycle(optimizer, num_training_steps):\n",
    "    \"\"\"Simple single-cycle scheduler. Not including paper/fastai three-phase things or asymmetry.\"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_training_steps / 2:\n",
    "            return float(current_step / (num_training_steps / 2))\n",
    "        else:\n",
    "            return float(2 - current_step / (num_training_steps / 2))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, -1)\n",
    "\n",
    "\n",
    "# adam_fn = partial(torch.optim._functional.adam, amsgrad=False, beta1=0.9, beta2=0.98, weight_decay=0, eps=1e-6, maximize=False)\n",
    "optimizer = bnb.optim.Adam8bit(\n",
    "    model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-6\n",
    ")\n",
    "schedular = get_one_cycle(\n",
    "    optimizer, int(NUM_TRAIN_EPOCHS * len(dataset_train) / TRAINING_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/hf_trainer/\",\n",
    "    logging_dir=\"./data/hf_trainer/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAINING_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TRAINING_BATCH_SIZE,\n",
    "    save_steps=200,\n",
    "    logging_steps=2,\n",
    "    gradient_accumulation_steps=num_accumulation_steps,\n",
    "    # eval_steps=100,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # prediction_loss_only=True,\n",
    "    # learning_rate=LEARNING_RATE,\n",
    "    save_total_limit=5,\n",
    "    dataloader_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    disable_tqdm=True,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"wandb\",\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=dataset_train,\n",
    "    optimizers=[optimizer, schedular],\n",
    "    # eval_dataset=dataset_eval,\n",
    "    callbacks=[MyCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_exit():\n",
    "    print(\"Saving model on exit...\")\n",
    "    torch.save(model.state_dict(),  savedir / \"model_on_exit.torch\")\n",
    "\n",
    "\n",
    "atexit.register(save_on_exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "torch.save(model.state_dict(), savedir / \"trained_model.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_idx, ds in enumerate([dataset_train, dataset_eval]):\n",
    "    print(f\"--- dataset {ds_idx} ---\\n\")\n",
    "    for i in range(20):\n",
    "        data = collate([ds[i]])\n",
    "        # data = collate([ds[random.randint(0, len(dataset_eval) - 1)]])\n",
    "        logits = model.predict(\n",
    "            data[\"masked_labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "        )[0]\n",
    "        print(decode(data[\"labels\"].detach().tolist()))\n",
    "        print(decode(data[\"masked_labels\"].detach().tolist()))\n",
    "        print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without masking (checking it can encode/decode input)\n",
    "examples = [\n",
    "    \"Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you?\",\n",
    "    \"it seems that this can output its input pretty well, as long as the input is of a decent length, but for short sentences it seems to not be good at all, this is very interesting\",\n",
    "    \"Hi, how are you doin? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you?\",\n",
    "    \"Interesting, it seems to fail even for the longer input, if it's very repetitive. I wonder what's going on there. Will this work better? Is it more like wiki text? that's interesting o.O.\",\n",
    "    \"alain connes (; born 1 april 1947) is a french mathematician, and a theoretical physicist, known for his contributions to the study of operator algebras and no\",\n",
    "    \"peter connes (; born april fools) is a french mathematician, and a masterful physicist, known for his many many contributions to the study of operator algebras and no\",\n",
    "    \"Hello, world!\",\n",
    "    \"One two three\",\n",
    "    \"the 2022 fa women's league cup\",\n",
    "    \"badreddine assouar (born may 5, 1974) is a physicist,\",\n",
    "]\n",
    "for example in examples:\n",
    "    data = collate([tokenize(example)])\n",
    "    logits = model.predict(\n",
    "        data[\"labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "    )[0]\n",
    "    print(decode(data[\"labels\"].detach().tolist()))\n",
    "    print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with masking\n",
    "for example in examples:\n",
    "    # data = collate([tokenize(example), dataset_train[40]])\n",
    "    data = collate([tokenize(example)], masking_probability=0.15)\n",
    "    # data = collate([ds[random.randint(0, len(dataset_eval) - 1)]])\n",
    "    logits = model.predict(\n",
    "        data[\"masked_labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "    )[0]\n",
    "    print(decode(data[\"labels\"].detach().tolist()))\n",
    "    print(decode(data[\"masked_labels\"].detach().tolist()))\n",
    "    print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
