{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run on full wiki text with this model, to see if gets better at masking\n",
    "\n",
    "%env WANDB_PROJECT=PACNLM\n",
    "\n",
    "\n",
    "import logging\n",
    "import atexit\n",
    "import random\n",
    "import string\n",
    "\n",
    "from magic_timer import MagicTimer\n",
    "from magic_timer.format_seconds import format_seconds\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from tokenizers import normalizers\n",
    "from transformers import Trainer, TrainingArguments, PrinterCallback\n",
    "\n",
    "import transformers.trainer as trainer\n",
    "from transformers.trainer import SequentialSampler\n",
    "\n",
    "\n",
    "def sampler_monkey_patch(dataset, generator):\n",
    "    # When the dataset size is large, I've measured:\n",
    "    # RandomSampler -> ~50 samples/sec\n",
    "    # SequentialSampler -> ~500 samples/sec\n",
    "    # So this patch is to get a nearly 10x speedup...\n",
    "    # This has got training time on wikipedia from 1+ day to 3 hrs...\n",
    "    print(\"Monkey patching random sampler...\")\n",
    "    return SequentialSampler(dataset)\n",
    "\n",
    "\n",
    "trainer.RandomSampler = sampler_monkey_patch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS = \"<cls>\"\n",
    "EOS = \"<eos>\"\n",
    "CHAR_PAD = \"<char_pad>\"\n",
    "UNK = \"<unk>\"\n",
    "NGRAM_PAD = \"<ngram_pad>\"\n",
    "MASK = \"<mask>\"\n",
    "\n",
    "SPECIAL_CHARS = {\n",
    "    CLS,\n",
    "    EOS,\n",
    "    CHAR_PAD,\n",
    "    UNK,\n",
    "    NGRAM_PAD,\n",
    "    MASK,\n",
    "}\n",
    "CHAR_TOKENS: list[str] = sorted(list(string.printable) + list(SPECIAL_CHARS))\n",
    "NGRAM_SIZE: int = 8\n",
    "NUM_ATTENTION_HEADS: int = 12\n",
    "HIDDEN_SIZE: int = 1536  # multiple of NUM_ATTENTION_HEADS, 768 default\n",
    "MAX_SEQ_LEN: int = NGRAM_SIZE * 20  # multiple of NGRAM_SIZE\n",
    "PROB_MASK: float = 0.15\n",
    "\n",
    "TRAINING_BATCH_SIZE = 64\n",
    "DATALOADER_NUM_WORKERS = 10\n",
    "LEARNING_RATE = 5e-5  # defaults to 5e-5\n",
    "\n",
    "num_chars = len(CHAR_TOKENS)\n",
    "char_to_idx = {c: i for i, c in enumerate(CHAR_TOKENS)}\n",
    "idx_to_char = {i: c for i, c in enumerate(CHAR_TOKENS)}\n",
    "\n",
    "normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(seq: str):\n",
    "    seq = normalizer.normalize_str(seq)\n",
    "    seq = [CLS] + list(seq)\n",
    "    # Pad such that len(seq) is divisible by NGRAM_SIZE\n",
    "    if len(seq) % NGRAM_SIZE > 0:\n",
    "        seq += [CHAR_PAD] * (NGRAM_SIZE - (len(seq) % NGRAM_SIZE))\n",
    "    seq += [EOS] * NGRAM_SIZE\n",
    "    return torch.tensor(\n",
    "        [char_to_idx[c] if c in char_to_idx else char_to_idx[UNK] for c in seq]\n",
    "    )\n",
    "\n",
    "\n",
    "def collate(tokenized_seqs: list[torch.tensor], masking_probability: float = PROB_MASK):\n",
    "    \"\"\"Pad short seqs, truncate long seqs.\"\"\"\n",
    "    tokenized_seqs = [x[:MAX_SEQ_LEN] for x in tokenized_seqs]\n",
    "    max_len = max(x.shape[-1] for x in tokenized_seqs)\n",
    "    labels = torch.full(\n",
    "        size=[len(tokenized_seqs), max_len],\n",
    "        fill_value=char_to_idx[NGRAM_PAD],\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    attention_mask = torch.ones_like(labels)\n",
    "    for i, x in enumerate(tokenized_seqs):\n",
    "        labels[i, 0 : len(x)] = x\n",
    "        attention_mask[i, len(x) :] = 0\n",
    "    # Masking, on ngram level rather than char\n",
    "    masked_labels = labels.clone().detach()\n",
    "    for row_idx in range(masked_labels.shape[0]):\n",
    "        for ngram_idx in range(0, masked_labels.shape[1], NGRAM_SIZE):\n",
    "            if random.random() < masking_probability:\n",
    "                masked_labels[\n",
    "                    row_idx, ngram_idx : ngram_idx + NGRAM_SIZE\n",
    "                ] = char_to_idx[MASK]\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"masked_labels\": masked_labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.examples = load_dataset(\n",
    "            \"wikipedia\",\n",
    "            \"20220301.en\",\n",
    "            split=split,\n",
    "            cache_dir=\"/media/bigdata/datasets/\",\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.examples[i][\"text\"]\n",
    "        if not type(text) == str:\n",
    "            text = \"\"\n",
    "        return tokenize(text)\n",
    "\n",
    "\n",
    "dataset_train = MyDataset(split=f\"train[:-{TRAINING_BATCH_SIZE}]\")\n",
    "dataset_eval = MyDataset(split=f\"train[-{TRAINING_BATCH_SIZE}:]\")\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(\n",
    "        load_dataset(\n",
    "            \"wikipedia\",\n",
    "            \"20220301.en\",\n",
    "            split=\"train\",\n",
    "            cache_dir=\"/media/bigdata/datasets/\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # An embedding table for each slot in the the ngram, (e.g. 0, 1, 2 for a NGRAM_SIZE=3).\n",
    "        self.ngram_embedding_tables = [\n",
    "            torch.nn.Embedding(\n",
    "                num_embeddings=num_chars,\n",
    "                embedding_dim=HIDDEN_SIZE,\n",
    "                padding_idx=char_to_idx[NGRAM_PAD],\n",
    "            )\n",
    "            for _ in range(NGRAM_SIZE)\n",
    "        ]\n",
    "        self.language_model = RobertaForMaskedLM(\n",
    "            config=RobertaConfig(\n",
    "                vocab_size=2,  # won't use\n",
    "                hidden_size=HIDDEN_SIZE,  # default 768\n",
    "                max_position_embeddings=514,\n",
    "                num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers=6,\n",
    "                type_vocab_size=1,\n",
    "                attention_probs_dropout_prob=0,\n",
    "                hidden_dropout_prob=0,\n",
    "            )\n",
    "        )\n",
    "        # To map from the lm embeddings back to the chars\n",
    "        self.ngram_prediction_heads = [\n",
    "            torch.nn.Linear(HIDDEN_SIZE, num_chars) for _ in range(NGRAM_SIZE)\n",
    "        ]\n",
    "\n",
    "    def forward(self, labels, masked_labels, attention_mask):\n",
    "        logits = self.predict(masked_labels, attention_mask)[0]\n",
    "        loss = self.get_loss(logits, labels)\n",
    "        return MaskedLMOutput(loss=loss, logits=logits)\n",
    "\n",
    "    def predict(self, labels, attention_mask):\n",
    "        input_embeddings = self.get_input_embeddings(labels)\n",
    "        lm_embeddings = self.language_model.roberta.forward(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=attention_mask[:, ::NGRAM_SIZE],\n",
    "        ).last_hidden_state\n",
    "        logits = self.get_predicted_char_logits(lm_embeddings)\n",
    "        return logits, lm_embeddings, input_embeddings\n",
    "\n",
    "    def get_loss(self, logits, labels):\n",
    "        return torch.nn.functional.cross_entropy(\n",
    "            logits.reshape(-1, num_chars), labels.reshape(-1)\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self, x_batch: torch.tensor):\n",
    "        result = []\n",
    "        for ngram_slot_idx in range(NGRAM_SIZE):\n",
    "            ngram_slot_embeddings = self.ngram_embedding_tables[ngram_slot_idx](\n",
    "                x_batch[:, ngram_slot_idx::NGRAM_SIZE]\n",
    "            )\n",
    "            result.append(ngram_slot_embeddings)\n",
    "        result = torch.stack(result).sum(dim=0)\n",
    "        return result\n",
    "\n",
    "    def get_predicted_char_logits(self, xbatch_lm_embeddings: torch.tensor):\n",
    "        \"\"\"Map from the lm embeddings back to the chars\"\"\"\n",
    "        result = []\n",
    "        for ngram_slot_idx in range(NGRAM_SIZE):\n",
    "            predicted_char = self.ngram_prediction_heads[ngram_slot_idx](\n",
    "                xbatch_lm_embeddings\n",
    "            )\n",
    "            result.append(predicted_char)\n",
    "        result = torch.concatenate(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        for x in self.ngram_embedding_tables:\n",
    "            x.to(*args, **kwargs)\n",
    "        self.language_model.to(*args, **kwargs)\n",
    "        for x in self.ngram_prediction_heads:\n",
    "            x.to(*args, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel()\n",
    "# model = torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(labels):\n",
    "    # To convert back to text\n",
    "    predicted_sentences = []\n",
    "    for sentence_ids in labels:\n",
    "        chars = []\n",
    "        for i in sentence_ids:\n",
    "            char = idx_to_char[i]\n",
    "            # `char in chars[-1:]` is to compare to the last char,\n",
    "            # that also works when there are no no chars..\n",
    "            if char in SPECIAL_CHARS and char in chars[-1:]:\n",
    "                continue\n",
    "            chars.append(char)\n",
    "        predicted_sentences.append(\"\".join(chars))\n",
    "    return predicted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(PrinterCallback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.total_timer = None\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called after logging the last logs.\n",
    "        \"\"\"\n",
    "        eta = format_seconds(\n",
    "            (state.max_steps - state.global_step)\n",
    "            * (self.total_timer.time_elapsed() / state.global_step)\n",
    "        )\n",
    "        samples_per_second = (\n",
    "            state.global_step * TRAINING_BATCH_SIZE\n",
    "        ) / self.total_timer.time_elapsed()\n",
    "        print(\n",
    "            f\"Time elapsed: {self.total_timer}\"\n",
    "            f\" -- Steps: {state.global_step} / {state.max_steps}\"\n",
    "            f\" -- Estimated time left: {eta}\"\n",
    "            f\" -- Samples per second: {samples_per_second}\"\n",
    "        )\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(\"hi\")\n",
    "        if not self.total_timer:\n",
    "            self.total_timer = MagicTimer()\n",
    "        self.epoch_timer = MagicTimer()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/hf_trainer/\",\n",
    "    logging_dir=\"./data/hf_trainer/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=TRAINING_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TRAINING_BATCH_SIZE,\n",
    "    save_steps=5000,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_eval,\n",
    "    callbacks=[MyCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_exit():\n",
    "    print(\"Saving model on exit...\")\n",
    "    torch.save(model.state_dict(), \"./data/model_on_exit.torch\")\n",
    "\n",
    "\n",
    "atexit.register(save_on_exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "torch.save(model.state_dict(), \"./data/model.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_idx, ds in enumerate([dataset_train, dataset_eval]):\n",
    "    print(f\"--- dataset {ds_idx} ---\\n\")\n",
    "    for i in range(20):\n",
    "        data = collate([ds[i]])\n",
    "        # data = collate([ds[random.randint(0, len(dataset_eval) - 1)]])\n",
    "        logits = model.predict(\n",
    "            data[\"masked_labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "        )[0]\n",
    "        print(decode(data[\"labels\"].detach().tolist()))\n",
    "        print(decode(data[\"masked_labels\"].detach().tolist()))\n",
    "        print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without masking (checking it can encode/decode input)\n",
    "examples = [\n",
    "    \"Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you?\",\n",
    "    \"it seems that this can output its input pretty well, as long as the input is of a decent length, but for short sentences it seems to not be good at all, this is very interesting\",\n",
    "    \"Hi, how are you doin? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you? Hi, how are you?\",\n",
    "    \"Interesting, it seems to fail even for the longer input, if it's very repetitive. I wonder what's going on there. Will this work better? Is it more like wiki text? that's interesting o.O.\",\n",
    "    \"alain connes (; born 1 april 1947) is a french mathematician, and a theoretical physicist, known for his contributions to the study of operator algebras and no\",\n",
    "    \"peter connes (; born april fools) is a french mathematician, and a masterful physicist, known for his many many contributions to the study of operator algebras and no\",\n",
    "    \"Hello, world!\",\n",
    "    \"One two three\",\n",
    "    \"the 2022 fa women's league cup\",\n",
    "    \"badreddine assouar (born may 5, 1974) is a physicist,\",\n",
    "]\n",
    "for example in examples:\n",
    "    data = collate([tokenize(example)])\n",
    "    logits = model.predict(\n",
    "        data[\"labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "    )[0]\n",
    "    print(decode(data[\"labels\"].detach().tolist()))\n",
    "    print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with masking\n",
    "for example in examples:\n",
    "    # data = collate([tokenize(example), dataset_train[40]])\n",
    "    data = collate([tokenize(example)], masking_probability=0.15)\n",
    "    # data = collate([ds[random.randint(0, len(dataset_eval) - 1)]])\n",
    "    logits = model.predict(\n",
    "        data[\"masked_labels\"].to(device), data[\"attention_mask\"].to(device)\n",
    "    )[0]\n",
    "    print(decode(data[\"labels\"].detach().tolist()))\n",
    "    print(decode(data[\"masked_labels\"].detach().tolist()))\n",
    "    print(decode(logits.argmax(axis=2).detach().tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
